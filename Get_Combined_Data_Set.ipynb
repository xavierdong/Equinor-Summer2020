{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QQaoAFkYlt_E",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import urllib\n",
    "import codecs\n",
    "from datetime import timedelta\n",
    "from pandasql import sqldf\n",
    "\n",
    "_sas_url = \"https://datavillagesa.blob.core.windows.net/volve?sv=2018-03-28&sr=c&sig=MgaLzfQcNK%2B%2FdMb3EyoF83U%2BvgKzQaxMo8O0ZbFhE6s%3D&se=2020-08-16T16%3A56%3A56Z&sp=rl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CzZzdDqPl3hT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583.0
    },
    "outputId": "50137928-1782-4ddb-c849-cbca01b5cea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.6/dist-packages (12.3.2)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (2.9.2)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (1.7.0)\n",
      "Requirement already satisfied: msrest>=0.6.10 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob) (0.6.17)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.12.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from azure-core<2.0.0,>=1.6.0->azure-storage-blob) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (2020.6.20)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (0.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.6.0->azure-storage-blob) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.6.0->azure-storage-blob) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.6.0->azure-storage-blob) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob) (3.1.0)\n",
      "Requirement already satisfied: tabula-py in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from tabula-py) (1.0.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tabula-py) (1.18.5)\n",
      "Requirement already satisfied: distro in /usr/local/lib/python3.6/dist-packages (from tabula-py) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->tabula-py) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.25.3->tabula-py) (1.12.0)\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
      "Requirement already satisfied: pandasql in /usr/local/lib/python3.6/dist-packages (0.7.3)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.3.18)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.0.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pandasql) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pandasql) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pandasql) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage-blob\n",
    "!pip install tabula-py\n",
    "!pip install bs4\n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NSh69kHKl6Gj",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "63426195-576a-4cf4-e2b5-addfc00b553a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LQkXLrDSmVhJ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import lxml.html as lh\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "from os.path import isfile, splitext\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def convert_html_to_dataframe(html_path_list, delete=False):\n",
    "\n",
    "  html_operations_df_list = []\n",
    "  for html_path in html_path_list:\n",
    "    if not isfile(html_path):\n",
    "      print(str(html_path) + \" is not a valid path.\")\n",
    "      continue\n",
    "\n",
    "    curr_path = Path(html_path)\n",
    "\n",
    "     # String manipulation on file to get Name and Date\n",
    "    file_name = splitext(curr_path.parts[-1])[0]  # Removes PDF ending\n",
    "    #Regex for the datetime (right now leaves the format with underscores)\n",
    "    log_date = re.findall(\"([12]\\d{3}_(0[1-9]|1[0-2])_(0[1-9]|[12]\\d|3[01]))\",\n",
    "                              file_name)[0][0]  # re.findall returns a tuple in a list (need for element of tuple)\n",
    "\n",
    "    # Need the name of the wellbore\n",
    "    wellbore_name = file_name.replace(\"_\" + log_date, \"\")\n",
    "    \n",
    "    # parsing html contents\n",
    "    # html_path = \"/content/drive/My Drive/15_9_F_10_2009_04_10.html\"\n",
    "\n",
    "    soup = BSoup(open(html_path), \"html.parser\")\n",
    "    op_table = soup.find(\"table\", {\"id\": \"operationsInfoTable\"})\n",
    "\n",
    "    if op_table is None:\n",
    "        print(\"Parsing HTMLs: cannot find operations table for\", html_path)\n",
    "        continue\n",
    "    \n",
    "    headers = op_table.find(\"thead\").findAll(\"th\")\n",
    "    headers = [header.text for header in headers]\n",
    "    # print(headers)\n",
    "\n",
    "    rows = [[col.text for col in row.findAll(\"td\")] for row in op_table.find(\"tbody\").findAll(\"tr\")]\n",
    "    \n",
    "    curr_df = pd.DataFrame(rows, columns=headers)\n",
    "    #Maybe a conditional \n",
    "    log_date_list = [log_date] * curr_df.shape[0]\n",
    "    wellbore_name_list = [wellbore_name] * curr_df.shape[0]\n",
    "    curr_df['log_date'] = log_date_list\n",
    "    curr_df['wellbore_name'] = wellbore_name_list\n",
    "    html_operations_df_list.append(curr_df)\n",
    "  \n",
    "  # for df in html_operations_df_list:\n",
    "  #   print(\"(\" + str(len(df.columns)) + \", \" + str(df.columns) + \")\")\n",
    "  # print(html_operations_df_list[0].head())\n",
    "\n",
    "  complete_df = pd.concat(html_operations_df_list, ignore_index=True)\n",
    "  complete_df['log_date'] = complete_df['log_date'].map(lambda x: x.replace(\"_\", \"-\"))\n",
    "  complete_df['wellbore_name'] = complete_df['wellbore_name'].map(lambda x: x.replace(\"_\", \"-\").replace(\"-\", \"_\", 1))\n",
    "\n",
    "  if delete:\n",
    "    for path in html_path_list:\n",
    "      os.remove(path)\n",
    "  return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hClkI7qUmd9n",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def filter_wellbore_name(log_element):\n",
    "    new_path = Path(log_element)\n",
    "    file_name = os.path.splitext(new_path.parts[-1])[0]\n",
    "    log_date = re.findall(\"([12]\\d{3}_(0[1-9]|1[0-2])_(0[1-9]|[12]\\d|3[01]))\",\n",
    "                          file_name)[0][0]  # re.findall returns a tuple in a list (need for element of tuple)\n",
    "\n",
    "    # #Need the name of the wellbore\n",
    "    wellbore_name_blob = file_name.replace(\"_\" + log_date, \"\")\n",
    "    wellbore_name_blob = wellbore_name_blob.replace(\"_\", \"-\").replace('-', '_', 1)\n",
    "\n",
    "    return wellbore_name_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3oYD5lDRmkGw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "def fetch_html_daily_drilling_logs(out_dir, sas_dict, wellbore_name=None, log_date=None):\n",
    "    \"\"\"\n",
    "    Takes html daily drilling well logs from the Volve dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_dir : string\n",
    "      Path where the PDF files will be downloaded\n",
    "\n",
    "    wellbore_name : string\n",
    "\n",
    "    log_date : string\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    container = ContainerClient.from_container_url(container_url=_sas_url)\n",
    "\n",
    "    daily_log_html_path = 'Well_technical_data/Daily Drilling Report - HTML Version/'\n",
    "    # Well_technical_data/Daily Drilling Report - HTML Version/\n",
    "\n",
    "    daily_log_list = []  # Gives list of paths\n",
    "    path_list = []\n",
    "\n",
    "    # for i in range(len(blob_list)):\n",
    "    #     curr_path = Path(blob_list[i]['name'])\n",
    "    #     directory_path = curr_path.parts[:-1]\n",
    "    #     file_name = curr_path.parts[-1]\n",
    "\n",
    "    #     tot_path = \"\"\n",
    "    #     for path_element in directory_path:  # Can use join\n",
    "    #         tot_path += path_element\n",
    "    #         tot_path += \"/\"\n",
    "    #     # print(\"tot_path: \" + str(tot_path))\n",
    "\n",
    "    #     if (tot_path == daily_log_html_path):\n",
    "    #         daily_log_list.append(curr_path)\n",
    "\n",
    "    # filtered_daily_log_list = []\n",
    "    # if wellbore_name is not None:\n",
    "\n",
    "    #     for log_element in daily_log_list:\n",
    "    #         wellbore_name_blob = filter_wellbore_name(log_element)\n",
    "    #         # print(\"(\" + wellbore_name_blob + \", \" + wellbore_name + \")\")\n",
    "    #         if wellbore_name_blob == wellbore_name:\n",
    "    #             filtered_daily_log_list.append(log_element)\n",
    "    # else:\n",
    "    #     filtered_daily_log_list = daily_log_list\n",
    "    data = sas_dict.get(wellbore_name, None)\n",
    "    if data is None:\n",
    "        print(\"Cannot find paths for\", wellbore_name)\n",
    "        return\n",
    "\n",
    "    filtered_daily_log_list = data.get(\"html_reports\")\n",
    "    # print(\"Number of daily logs taken: \" + str(len(filtered_daily_log_list)))\n",
    "\n",
    "    print(\"Number of daily logs taken: \" + str(len(filtered_daily_log_list)))\n",
    "\n",
    "    for path in filtered_daily_log_list:\n",
    "        path = Path(path)\n",
    "        blob_client = container.get_blob_client(str(path))\n",
    "        download = blob_client.download_blob()\n",
    "        path_list.append(os.path.join(out_dir, path.parts[-1]))\n",
    "        with open(os.path.join(out_dir, path.parts[-1]), 'wb') as f:\n",
    "            f.write(download.readall())\n",
    "\n",
    "    print(\"Finished Getting Azure HTML Data\")\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6MVSFbcAmu6j",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def combine_run_and_realtime_drilling(real_time_drilling_data, run_data):\n",
    "    \"\"\"\n",
    "    Uses pandasql to perform an inner join on the real_time_drilling_data and the\n",
    "    run_data with some data preprocessing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    real_time_drilling_data: Pandas Dataframe\n",
    "    Contains the realtime drilling data for a wellbore\n",
    "    run_data\n",
    "    Contains the run (BHA) data for a set of wellbore names including the run\n",
    "    number, the lowest depth, and the highest depth for that run number\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    real_time_drilling_data['wellbore_name'] = real_time_drilling_data['nameWellbore'].map(lambda x: (x[:8]).replace(\"/\", \"_\"))\n",
    "\n",
    "    q = \"\"\"\n",
    "        SELECT *\n",
    "        FROM real_time_drilling_data rtdd\n",
    "        INNER JOIN run_data rd\n",
    "          ON rtdd.DMEA >= rd.lower_bound\n",
    "          AND rtdd.DMEA <= rd.upper_bound\n",
    "          AND rtdd.wellbore_name == rd.wellbore_name\n",
    "    \"\"\"\n",
    "\n",
    "    joined_df = sqldf(q)\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UHJ9vN9gmyyJ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def group_wellbore_run_data(output_dir, input_wellbore_names):\n",
    "    \"\"\"\n",
    "    Gets the wellbore run data from the volve Inventory file and creates a new\n",
    "    dataframe that is groups the run numbers taking the minimum and maximum\n",
    "    depth for the lower_bound and upper_bound columns\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir: string\n",
    "        Output path returned after fetching the volve inventory data\n",
    "    input_wellbore_names: list\n",
    "      This is a list of the wellbore names that one wants to extract from the\n",
    "      Volve Inventory file\n",
    "    \"\"\"\n",
    "\n",
    "    wellbore_run_df_list = []\n",
    "    volve_inventory = pd.ExcelFile(output_dir)\n",
    "\n",
    "    volve_sheet_names = volve_inventory.sheet_names\n",
    "\n",
    "    for input_name in input_wellbore_names:\n",
    "        data = volve_inventory.parse(input_name)\n",
    "        data.columns = data.iloc[0]\n",
    "        data = data.drop(index=0, axis=1)\n",
    "        # print(data.columns)\n",
    "\n",
    "        run_data_dict = {'FOLDER': data['FOLDER'], 'Run No.': data['Run No.'], 'Interval': data['Interval']}\n",
    "        run_data = pd.DataFrame(data=run_data_dict)\n",
    "\n",
    "        run_data = run_data[run_data['Interval'] != 'TIME'] #Could also use regex\n",
    "\n",
    "        run_data['lower_bound'] = run_data['Interval'].map(lambda x: str(x).split('-')[0])\n",
    "        run_data['upper_bound'] = run_data['Interval'].map(\n",
    "            lambda x: x if len(str(x).split(\"-\")) == 1 else str(x).split(\"-\")[1].replace(\" m\", \"\"))\n",
    "\n",
    "        # Remove values if interval is not defined\n",
    "        run_data = run_data.dropna(subset=['Interval', 'Run No.']) #need to worry about the TIME thing\n",
    "        run_data = run_data[run_data['Run No.'].map(lambda run_num: len(\n",
    "            str(run_num).split(\"-\")) == 1)]  # Gets rid of the 1-4 b/c it is shown in other parts of the data\n",
    "        run_data = run_data[\n",
    "            run_data['FOLDER'] == 'LWD_EWL']  # This can be used to get rid of values for production logs if needed\n",
    "        run_data['Run No.'] = run_data['Run No.'].map(\n",
    "            lambda run_num: str(run_num))  # Fixes problems with strings vs. integers in data\n",
    "\n",
    "        run_grouped_data = run_data.groupby(by='Run No.').agg(\n",
    "            {'lower_bound': 'min', 'upper_bound': 'max'}).reset_index()\n",
    "        input_name = input_name.replace(\" \", \"\")\n",
    "        run_grouped_data['wellbore_name'] = [input_name] * run_grouped_data.shape[0]\n",
    "\n",
    "        wellbore_run_df_list.append(run_grouped_data)\n",
    "\n",
    "    return wellbore_run_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "km9Kny5am9TG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def combine_time_and_html_log_data(real_time_drilling_data, daily_log_data):\n",
    "    \"\"\"\n",
    "    Combines the real time drilling data with the operations table from\n",
    "    the daily log drilling data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    real_time_drilling_data : Pandas Dataframe\n",
    "        WITSML Realtime drilling time from the Volve dataset taken from Azure\n",
    "        Blob Storage converted into a Pandas Dataframe\n",
    "    daily_log_data : Pandas Dataframe\n",
    "        Operations data tables from the daily drilling reports in the Volve dataset\n",
    "        converted intoa Pandas Dataframe from Azure Blob Storage\n",
    "    \"\"\"\n",
    "\n",
    "    #Parse datetimes of TIME column into two seperate columns\n",
    "    time = 'TIME'\n",
    "    # if time not in real_time_drilling_data:\n",
    "    #     time = \"Time\"\n",
    "    daily_log_data = daily_log_data.rename(columns={\"Start time\": \"Start_time\", \"End time\": \"End_time\"})\n",
    "\n",
    "    real_time_drilling_data['original_time'] = real_time_drilling_data['TIME']\n",
    "    real_time_drilling_data.astype({'TIME':'datetime64[ns]'})\n",
    "    real_time_drilling_data['TIME'] = pd.to_datetime(real_time_drilling_data['TIME']) + timedelta(hours=8)\n",
    "\n",
    "    real_time_drilling_data['log_date'] = real_time_drilling_data['TIME'].map(lambda x: x.date())\n",
    "    real_time_drilling_data['current_time'] = real_time_drilling_data['TIME'].map(lambda x: x.strftime(\"%H:%M:%S\"))\n",
    "    # strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # print(real_time_drilling_data['TIME'].head())\n",
    "    # print(real_time_drilling_data['log_date'].head())\n",
    "    # print(real_time_drilling_data['current_time'].head())\n",
    "    # print(real_time_drilling_data['original_time'].head())\n",
    "\n",
    "    # real_time_drilling_data['log_date'] = real_time_drilling_data[time].map(lambda x: x[:10])\n",
    "    # real_time_drilling_data['current_time'] = real_time_drilling_data[time].map(lambda x: x[11:-5])\n",
    "    #Makes new wellbore_name column to be compatible with the daily_log_date\n",
    "    real_time_drilling_data['wellbore_name'] = real_time_drilling_data['nameWellbore'].map(lambda x: (x[:8]).replace(\"/\", \"_\"))\n",
    "\n",
    "    # print(real_time_drilling_data.head())\n",
    "    # print(real_time_drilling_data.columns)\n",
    "    # print(\"--------------------------------\")\n",
    "    # print(daily_log_data)\n",
    "    # print(daily_log_data.columns)\n",
    "\n",
    "    # print(daily_log_data.head())\n",
    "    q = \"\"\"\n",
    "      SELECT *\n",
    "      FROM real_time_drilling_data rtdd\n",
    "      INNER JOIN daily_log_data dld \n",
    "        ON (rtdd.current_time BETWEEN\n",
    "          dld.Start_time AND dld.End_time)\n",
    "          AND (rtdd.log_date == dld.log_date)\n",
    "          AND (rtdd.wellbore_name == dld.wellbore_name)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing SQL querry\")\n",
    "    joined_df = sqldf(q)\n",
    "    print(\"Finished join\")\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZjEno6AknCtu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# from conversion import merge_xml_to_csv, get_drill_log_tables, convert_html_to_dataframe\n",
    "# from data_preprocessing import group_wellbore_run_data, combine_time_and_html_log_data, combine_run_and_realtime_drilling\n",
    "\n",
    "\n",
    "def fetch_sas_paths(path_checker_csv):\n",
    "    \"\"\"\n",
    "    Fetches paths to the 'WITSML Realtime drilling data',\n",
    "    'Well_technical_data/Daily Drilling report - PDF Version', and\n",
    "    'Well_technical_data/Daily Drilling Report - HTML Version'\n",
    "    blobs\n",
    "\n",
    "    :return:    dictionary of paths\n",
    "                    keys:    name of well\n",
    "                    values:  dictionary\n",
    "                        keys: \"drill\", \"pdf_reports\", \"html_reports\"\n",
    "                        values: list of paths\n",
    "    :rtype:     dict[str, dict[str, list[str]]\n",
    "    \"\"\"\n",
    "\n",
    "    container = ContainerClient.from_container_url(container_url=_sas_url)\n",
    "    \n",
    "    daily_log_pdf_path = 'Well_technical_data/Daily Drilling report - PDF Version/'\n",
    "    daily_log_html_path = 'Well_technical_data/Daily Drilling Report - HTML Version/'\n",
    "\n",
    "    drilling_paths = {}\n",
    "    pdf_reports_paths = {}\n",
    "    html_reports_paths = {}\n",
    "    valid_drill_subs = __get_valid_drill_sub(path_checker_csv)\n",
    "\n",
    "    print(\"Fetch: retrieving list of blobs\")\n",
    "    blob_list = list(container.list_blobs())\n",
    "\n",
    "    print(\"Fetch: filtering list of blobs\")\n",
    "    for blob in blob_list:\n",
    "\n",
    "        name = blob.name\n",
    "        if 'WITSML Realtime drilling data' in name \\\n",
    "                and 'log' in name \\\n",
    "                and os.path.splitext(name)[1] == '.xml' \\\n",
    "                and (valid_drill_subs is None or __is_valid_drill(name, valid_drill_subs)):\n",
    "\n",
    "            # extract well code\n",
    "            well = __get_well_name_from_drill_sub(name)\n",
    "            # append to data\n",
    "            well_drills = drilling_paths.get(well, [])\n",
    "            well_drills.append(name)\n",
    "            drilling_paths[well] = well_drills\n",
    "\n",
    "        elif daily_log_pdf_path in name:\n",
    "\n",
    "            # extract well code\n",
    "            well = __get_well_name_from_report_blob(name, \".pdf\")\n",
    "            # append to data\n",
    "            well_reports = pdf_reports_paths.get(well, [])\n",
    "            well_reports.append(name)\n",
    "            pdf_reports_paths[well] = well_reports\n",
    "\n",
    "        elif daily_log_html_path in name:\n",
    "            # extract well code\n",
    "            well = __get_well_name_from_report_blob(name, \".html\")\n",
    "            # append to data\n",
    "            well_reports = html_reports_paths.get(well, [])\n",
    "            well_reports.append(name)\n",
    "            html_reports_paths[well] = well_reports\n",
    "\n",
    "    return {\n",
    "        key: {\n",
    "            \"drill\": drilling_paths[key],\n",
    "            \"pdf_reports\": pdf_reports_paths.get(key, []),\n",
    "            \"html_reports\": html_reports_paths.get(key, [])\n",
    "        } for key in drilling_paths.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_all_drilling_data(out_dir, sas_dict, well_name, override=False):\n",
    "    \"\"\"\n",
    "    fetches all realtime drilling data for one well\n",
    "    :param out_dir: pre-existing output directory\n",
    "    :type out_dir: str\n",
    "    :param sas_dict: dictionary fetched from fetch_sas_path function\n",
    "    :type sas_dict: dict[str, dict[list[str]]]\n",
    "    :param well_name: name of well to fetch data for\n",
    "    :type well_name: str\n",
    "    :return: paths to the output files\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    data = sas_dict.get(well_name, None)\n",
    "    if data is None:\n",
    "        print(\"Cannot find paths for\", well_name)\n",
    "        return\n",
    "\n",
    "    sub_folders = {}\n",
    "    for path in data.get(\"drill\"):\n",
    "        key = __get_file_name(Path(path))\n",
    "        sub = sub_folders.get(key, [])\n",
    "        sub.append(path)\n",
    "        sub_folders[key] = sub\n",
    "\n",
    "    out_paths = []\n",
    "    print(sub_folders)\n",
    "    for sub, paths in sub_folders.items():\n",
    "        out_path, dict_uids = fetch_sub_drilling_data(out_dir, sub, paths, override)\n",
    "        out_paths.append(out_path)\n",
    "\n",
    "    return out_paths\n",
    "\n",
    "\n",
    "def fetch_sub_drilling_data(out_dir, filename, sub_paths, override=False):\n",
    "    \"\"\"Fetch all drilling data for one sub folder and convert into a single .csv file\n",
    "    :param out_dir: pre-existing file output directory\n",
    "    :type out_dir: str\n",
    "    :param filename: output file name (must be generated from __get_file_name)\n",
    "    :type filename: str\n",
    "    :param sub_paths: list of paths to the individual blobs in the sub folder\n",
    "    :type sub_paths: list[str]\n",
    "    :param override: override old file if same file name found\n",
    "    :type override: bool\n",
    "    \"\"\"\n",
    "\n",
    "    # parse information from target\n",
    "    # t_well = \"\"\n",
    "    #\n",
    "    # target = re.match(\"\\S+volve/(\\S+)\\?\\S+\", target).group(1).replace(\"%20\", \" \").replace(\"%24\", \"$\")\n",
    "    # target = Path(target)\n",
    "    #\n",
    "    # t_well = _get_well_name(target.parts[1])\n",
    "    # t_name = _get_file_name(target, full_path=False)\n",
    "\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    if os.path.isfile(out_path) and not override:\n",
    "        print(\"Initiate fetch:\", filename, \"failed = file already exists\")\n",
    "        return out_path, None\n",
    "\n",
    "    print(\"Initiating fetch: drilling data\")\n",
    "    container = ContainerClient.from_container_url(container_url=_sas_url)\n",
    "\n",
    "    print(\"Fetch: downloading and converting files in directory =\", out_dir)\n",
    "    c_name = __get_file_name(Path(sub_paths[0]))\n",
    "    for path in sub_paths:\n",
    "        path = Path(path)\n",
    "        blob_client = container.get_blob_client(str(path))\n",
    "        download = blob_client.download_blob()\n",
    "\n",
    "        with open(os.path.join(out_dir, path.parts[-1]), 'w') as f:\n",
    "            f.write(download.readall().decode(\"utf-8\"))\n",
    "\n",
    "    if os.path.isfile(os.path.join(out_dir, c_name)) and override:\n",
    "        print(\"Fetch: preparing to override\", c_name)\n",
    "    out_path, dict_uids = merge_xml_to_csv(out_dir, output_name=c_name, del_temp=True, del_xml=True)\n",
    "\n",
    "    print(\"Fetch: finished\")\n",
    "    return out_path, dict_uids\n",
    "\n",
    "\n",
    "def fetch_daily_drilling_logs(out_dir, sas_dict, wellbore_name=None, log_date=None):\n",
    "    \"\"\"\n",
    "    Takes PDF daily drilling well logs from the Volve dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_dir : string\n",
    "      Path where the PDF files will be downloaded\n",
    "\n",
    "    wellbore_name : string\n",
    "\n",
    "    log_date : string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    container = ContainerClient.from_container_url(container_url=_sas_url)\n",
    "\n",
    "    data = sas_dict.get(wellbore_name, None)\n",
    "    if data is None:\n",
    "        print(\"Cannot find paths for\", wellbore_name)\n",
    "        return\n",
    "\n",
    "    filtered_daily_log_list = data.get(\"pdf_reports\")\n",
    "    print(\"Number of daily logs taken: \" + str(len(filtered_daily_log_list)))\n",
    "    out_paths = []\n",
    "\n",
    "    for path in filtered_daily_log_list:\n",
    "        path = Path(path)\n",
    "        blob_client = container.get_blob_client(str(path))\n",
    "        download = blob_client.download_blob()\n",
    "        out_path = os.path.join(out_dir, path.parts[-1])\n",
    "        out_paths.append(out_path)\n",
    "        with open(out_path, 'wb') as f:\n",
    "            f.write(download.readall())\n",
    "\n",
    "    print(\"Finished Getting Azure PDF Data\")\n",
    "    return out_paths\n",
    "\n",
    "\n",
    "def filter_wellbore_name(log_element):\n",
    "    new_path = Path(log_element)\n",
    "    file_name = os.path.splitext(new_path.parts[-1])[0]\n",
    "    log_date = re.findall(\"([12]\\d{3}_(0[1-9]|1[0-2])_(0[1-9]|[12]\\d|3[01]))\",\n",
    "                          file_name)[0][0]  # re.findall returns a tuple in a list (need for element of tuple)\n",
    "\n",
    "    # #Need the name of the wellbore\n",
    "    wellbore_name_blob = file_name.replace(\"_\" + log_date, \"\")\n",
    "    wellbore_name_blob = wellbore_name_blob.replace(\"_\", \"-\")\n",
    "\n",
    "    return wellbore_name_blob\n",
    "\n",
    "\n",
    "def fetch_wellbore_run_data(output_dir):\n",
    "    \"\"\"\n",
    "    Fetches the volve_inventory data from Azure Blob Storage\n",
    "    :param output_dir: path that where the data will be stored\n",
    "    :return: The path where the data is stored\n",
    "    \"\"\"\n",
    "    account_url = \"https://datavillagesa.blob.core.windows.net/\"\n",
    "    file_path = \"volve/Well_logs/VOLVE_INVENTORY.xlsx\"\n",
    "    sas_token = (\"?sv=2018-03-28&sr=c&sig=MgaLzfQcNK%2B%2FdMb3EyoF83U%2BvgKzQaxMo8O0ZbFhE6s%3D&se=2020-08-16T16%3A56%3A56Z&sp=rl\")\n",
    "    sas_url = account_url + file_path + _sas_url\n",
    "    blob_client = BlobClient.from_blob_url(sas_url)\n",
    "    with open(output_dir, \"wb\") as my_blob:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        my_blob.write(download_stream.readall())\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def fetch_html_daily_drilling_logs(out_dir, sas_dict, wellbore_name=None, log_date=None):\n",
    "    \"\"\"\n",
    "    Takes html daily drilling well logs from the Volve dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_dir : string\n",
    "      Path where the PDF files will be downloaded\n",
    "\n",
    "    wellbore_name : string\n",
    "\n",
    "    log_date : string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    container = ContainerClient.from_container_url(container_url=_sas_url)\n",
    "\n",
    "    daily_log_html_path = 'Well_technical_data/Daily Drilling Report - HTML Version/'\n",
    "    # Well_technical_data/Daily Drilling Report - HTML Version/\n",
    "\n",
    "    path_list = []\n",
    "\n",
    "    data = sas_dict.get(wellbore_name, None)\n",
    "    if data is None:\n",
    "        print(\"Cannot find paths for\", wellbore_name)\n",
    "        return\n",
    "\n",
    "    filtered_daily_log_list = data.get(\"html_reports\")\n",
    "\n",
    "    print(\"Number of daily logs taken: \" + str(len(filtered_daily_log_list)))\n",
    "\n",
    "    for path in filtered_daily_log_list:\n",
    "        path = Path(path)\n",
    "        blob_client = container.get_blob_client(str(path))\n",
    "        download = blob_client.download_blob()\n",
    "        path_list.append(os.path.join(out_dir, path.parts[-1]))\n",
    "        with open(os.path.join(out_dir, path.parts[-1]), 'wb') as f:\n",
    "            f.write(download.readall())\n",
    "\n",
    "    print(\"Finished Getting Azure HTML Data\")\n",
    "\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def __is_valid_drill(drill_path, valid_subs):\n",
    "    \"\"\"\n",
    "    Fetches paths to the 'WITSML Realtime drilling data' and 'Well_technical_data/Daily Drilling report - PDF Version'\n",
    "    blobs\n",
    "\n",
    "    :return:    dictionary of paths\n",
    "                    keys:    name of well\n",
    "                    values:  dictionary\n",
    "                        keys: \"drill\", \"pdf_reports\"\n",
    "                        values: list of paths\n",
    "    :rtype:     dict[str, dict[str, list]]\n",
    "    \"\"\"\n",
    "    path = Path(drill_path)\n",
    "\n",
    "    for valid in valid_subs:\n",
    "        if len(path.parts) > len(valid) + 1 and path.parts[1:len(valid) + 1] == valid:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def __get_valid_drill_sub(filename):\n",
    "    \"\"\"\n",
    "    parse and returns a list of valid real-time drill data subfolders\n",
    "    :param filename: path to .csv file containing said list\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO: update as necessary\n",
    "    # parse list of sub_folders --> currently only parses the 1st row of info\n",
    "\n",
    "    if filename is None:\n",
    "        return None\n",
    "\n",
    "    sub_folders = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        header = False\n",
    "        for row in reader:\n",
    "\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "\n",
    "            if header:\n",
    "                for col in row:\n",
    "                    if col:\n",
    "                        col = col.replace(\"\\\\\", \"/\").replace(\"\\'\", \"\").replace(\"\\\"\", \"\")\n",
    "                        sub_folders.append(Path(col).parts)\n",
    "                break\n",
    "    return sub_folders\n",
    "\n",
    "\n",
    "def __get_well_name_from_drill_sub(name):\n",
    "    target = [part for part in Path(name).parts if \"_$47$_\" in part][0].replace(\"_$47$_\", \"_\")\n",
    "    well = re.match(\"[ A-Za-z/-]+(.+)\", target, flags=re.DOTALL).group(1).replace(\" \", \"\")\n",
    "    well = well[:-1] if well[-1].isalpha() else well\n",
    "    return well\n",
    "\n",
    "\n",
    "def __get_well_name_from_report_blob(name, file_extension):\n",
    "    target = Path(name).parts[-1]\n",
    "    log_date = re.findall(\n",
    "        \"([12]\\d{3}_(0[1-9]|1[0-2])_(0[1-9]|[12]\\d|3[01]))\",\n",
    "        target\n",
    "    )[0][0]  # re.findall returns a tuple in a list (need for element of tuple)\n",
    "    well = target.replace(\"_\" + log_date, \"\").replace(file_extension, \"\")\n",
    "    well = well.rsplit(\"_\", well.count(\"_\") - 1)\n",
    "    well = '-'.join(well[:-1] if well[-1].isalpha() else well)\n",
    "    return well\n",
    "\n",
    "\n",
    "def __get_file_name(path, full_path=True):\n",
    "    return ('_'.join(path.parts[0:-1] if full_path else path.parts) + \".csv\").replace(\" \", \"_\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9IqWPgXPnjiI",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from os import listdir, remove, makedirs\n",
    "from os.path import isfile, join, exists, split, splitext\n",
    "from tabula import read_pdf\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.cElementTree as eTree\n",
    "\n",
    "\n",
    "def merge_xml_to_csv(in_dir, output_name=\"output.csv\", output_dir=None, del_temp=False, del_xml=False):\n",
    "    \"\"\"\n",
    "    Converts all .xml files in in_dir to .csv and merges them into one .csv file\n",
    "\n",
    "    Keyword arguments:\n",
    "    in_dir -- path to input directory containing files\n",
    "    output_name (optional) -- name of file as output\n",
    "                           -- default: \"output.csv\"\n",
    "    output_dir (optional) -- path to output directory\n",
    "                          -- default: same as in_dir\n",
    "    del_temp (optional) -- remove intermediate files\n",
    "                        -- default: False\n",
    "    del_xml (optional) -- remove .xml files used for conversion\n",
    "                       -- default: False\n",
    "\n",
    "    Return value -- tuple(str, list[str]):\n",
    "    returns tuple with values: path to output, list of uids for dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    fn_list = [join(in_dir, f) for f in listdir(in_dir) if f.endswith(\".xml\") and isfile(join(in_dir, f))]\n",
    "\n",
    "    if fn_list is None or len(fn_list) < 1:\n",
    "        return None\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = in_dir\n",
    "\n",
    "    # create output paths\n",
    "    filename, file_extension = splitext(output_name)\n",
    "    csv_out = join(output_dir, filename + '.csv')\n",
    "    dict_out, readme_out = _fetch_dictionary_path(csv_out)\n",
    "\n",
    "    # variable declarations\n",
    "    all_dict = {}\n",
    "    all_uid = []\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    csv_merge = open(csv_out, 'w')\n",
    "    csv_list = [xml_to_csv(fn) for fn in fn_list]\n",
    "\n",
    "    if del_xml:\n",
    "        for f_xml in fn_list:\n",
    "            remove(f_xml)\n",
    "\n",
    "    # merging process\n",
    "    for path, uid in csv_list:\n",
    "\n",
    "        # merge csv files\n",
    "        df = pd.read_csv(path)\n",
    "        data = data.append(df, ignore_index=True)\n",
    "\n",
    "        # merge the dictionaries\n",
    "        my_dict = fetch_dictionary(path)[uid]\n",
    "        all_dict[uid] = my_dict\n",
    "        all_uid.append(uid)\n",
    "\n",
    "        # remove temporary files if needed\n",
    "        if del_temp:\n",
    "\n",
    "            dict_file, readme_file = _fetch_dictionary_path(path)\n",
    "            remove(path)\n",
    "            remove(dict_file)\n",
    "            remove(readme_file)\n",
    "\n",
    "    csv_merge.close()\n",
    "\n",
    "    # write dictionary to file\n",
    "    all_uid = list(set(all_uid))\n",
    "\n",
    "    with open(dict_out, 'w') as file:\n",
    "        file.write(json.dumps(all_dict))\n",
    "    data.to_csv(csv_out, index=False)\n",
    "    write_readme_dictionary(readme_out, all_dict, all_uid)\n",
    "\n",
    "    print(\"Merged csv files to\", csv_out)\n",
    "    return csv_out, all_uid\n",
    "\n",
    "\n",
    "def xml_to_csv(filename, output=None):\n",
    "    \"\"\"\n",
    "    Converts xml files to csv files\n",
    "\n",
    "    Keyword arguments:\n",
    "    filename -- Path to xml file\n",
    "    output (optional) -- Path to destination with new filename\n",
    "                      -- default: same path and name as filename\n",
    "\n",
    "    Return value -- tuple(str, str):\n",
    "    returns tuple with values: path to output, uid for log\n",
    "    \"\"\"\n",
    "\n",
    "    if output is None:\n",
    "        output = splitext(filename)[0] + '.csv'\n",
    "\n",
    "    tree = eTree.parse(filename)\n",
    "\n",
    "    # fetching the namespace from the group\n",
    "    ns_m = re.match(r'{(.*)}', tree.getroot().tag)\n",
    "    ns = {'mw': ns_m.group(1)}\n",
    "\n",
    "    # initializing csv variables\n",
    "    csv_file = open(output, 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # fetching writing headers to file\n",
    "    headers_obj = tree.find(\".//mw:mnemonicList\", namespaces=ns)\n",
    "    headers = headers_obj.text.strip('\\n').split(',')\n",
    "\n",
    "    log_node = tree.find('.//mw:log', namespaces=ns)\n",
    "    log_attr = log_node.attrib\n",
    "    log_headers = list(log_attr.keys())\n",
    "\n",
    "    others = []\n",
    "    for node in log_node.iter():\n",
    "\n",
    "        head_m = re.match(r'{.*}(.*)', node.tag).group(1)\n",
    "        if head_m == 'logCurveInfo':\n",
    "            break\n",
    "        others.append((head_m, node.text))\n",
    "\n",
    "    others = others[1:]\n",
    "    others_val = [val for (header, val) in others]\n",
    "\n",
    "    headers = log_headers + [header for (header, val) in others] + headers\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    # order of headers: <log headers>, <other headers before logCurveInfo>, <others>\n",
    "    for data in tree.findall(\".//mw:data\", namespaces=ns):\n",
    "\n",
    "        data_write = data.text.strip('\\n').split(',')\n",
    "        data_write = list(log_attr.values()) + others_val + data_write\n",
    "        csv_writer.writerow(data_write)\n",
    "\n",
    "    csv_file.close()\n",
    "\n",
    "    # populating dictionary file\n",
    "    uid = 'NA'\n",
    "    if 'uid' in log_headers:\n",
    "        uid = log_attr['uid']\n",
    "    populate_dictionary(output, uid, tree.findall(\".//mw:logCurveInfo\", namespaces=ns))\n",
    "\n",
    "    print(\"Finished conversion:\", filename, \"to\", output)\n",
    "    return output, uid\n",
    "\n",
    "\n",
    "def populate_dictionary(filename, uid, lci_list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary and writes it as a json to a .txt file\n",
    "\n",
    "    Keyword arguments:\n",
    "    filename -- Path to location of .csv file\n",
    "    uid -- uid attribute for the log\n",
    "    lci_list -- list of logCurveInfo fetched from .xml file\n",
    "    \"\"\"\n",
    "\n",
    "    output, readme_out = _fetch_dictionary_path(filename)\n",
    "\n",
    "    # fetch list of dictionaries: one entry for each lci\n",
    "    parsed_dict = {}\n",
    "    for lci in lci_list:\n",
    "\n",
    "        lci_dict = {}\n",
    "        uid2 = lci.attrib['uid']\n",
    "\n",
    "        count = 0\n",
    "        for node in lci.iter():\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "\n",
    "            head_m = re.match(r'{.*}(.*)', node.tag).group(1)\n",
    "            lci_dict[head_m] = node.text\n",
    "\n",
    "        parsed_dict[uid2] = lci_dict\n",
    "\n",
    "    # writing dictionary to file\n",
    "    my_dict = {uid: parsed_dict}\n",
    "    with open(output, 'w') as file:\n",
    "        file.write(json.dumps(my_dict))\n",
    "\n",
    "    write_readme_dictionary(readme_out, my_dict, [uid])\n",
    "\n",
    "\n",
    "def write_readme_dictionary(filename, dictionary, uid_list):\n",
    "\n",
    "    head_written = False\n",
    "    csv_file = open(filename, 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    headers = []\n",
    "\n",
    "    for uid in uid_list:\n",
    "\n",
    "        sub = dictionary[uid]\n",
    "        for key, log_info in sub.items():\n",
    "\n",
    "            if head_written:\n",
    "                values = [log_info.get(k, '') for k in headers]\n",
    "                csv_writer.writerow([uid] + values)\n",
    "            else:\n",
    "                headers = list(log_info.keys())\n",
    "                csv_writer.writerow([\"uid-main\"] + headers)\n",
    "                head_written = True\n",
    "    csv_file.close()\n",
    "\n",
    "\n",
    "def fetch_dictionary(filename):\n",
    "    \"\"\"\n",
    "    Fetches the dictionary for the passed argument\n",
    "\n",
    "    Keyword arguments:\n",
    "    filename -- Path to the .csv file\n",
    "\n",
    "    Return value -- dict:\n",
    "    returns dictionary if file exists, None otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    f_path, o_path = _fetch_dictionary_path(filename)\n",
    "    if isfile(f_path):\n",
    "        file = open(f_path)\n",
    "        dictionary = dict(json.load(file))\n",
    "        file.close()\n",
    "        return dictionary\n",
    "    return None\n",
    "\n",
    "\n",
    "def _fetch_dictionary_path(csv_path):\n",
    "\n",
    "    _sub_dir = \"dict\"\n",
    "\n",
    "    _base_dir, _f_name = split(csv_path)\n",
    "    _sub_path = join(_base_dir, _sub_dir)\n",
    "\n",
    "    if not exists(_sub_path):\n",
    "        makedirs(_sub_path)\n",
    "\n",
    "    _filename = splitext(_f_name)[0]\n",
    "    return join(_sub_path, _filename + '.txt'), join(_sub_path, _filename + '_readme.csv')\n",
    "\n",
    "\n",
    "def get_drill_log_tables(file_path_list):\n",
    "    \"\"\"\n",
    "    Converts list of paths to PDF files to one single Pandas dataframe of the\n",
    "    operations data tables from each of the daily drilling log PDFS\n",
    "\n",
    "    Keyword arguments:\n",
    "    file_path_list -- list of paths to daily drilling report PDF files\n",
    "\n",
    "    Returns -- list of dataframes that are the operations data tables from the\n",
    "    daily drilling log PDFS\n",
    "    \"\"\"\n",
    "\n",
    "    operations_df_list = []\n",
    "\n",
    "    for pdf_path in file_path_list:\n",
    "        df_list = read_pdf(pdf_path)\n",
    "\n",
    "        table_num = len(df_list)\n",
    "        path = Path(pdf_path)\n",
    "\n",
    "        # String manipulation on file to get Name and Date\n",
    "        file_name = splitext(path.parts[-1])[0]  # Removes PDF ending\n",
    "        #Regex for the datetime\n",
    "        log_date = re.findall(\"([12]\\d{3}_(0[1-9]|1[0-2])_(0[1-9]|[12]\\d|3[01]))\",\n",
    "                              file_name)[0][0]  # re.findall returns a tuple in a list (need for element of tuple)\n",
    "\n",
    "        # Need the name of the wellbore\n",
    "        wellbore_name = file_name.replace(\"_\" + log_date, \"\")\n",
    "\n",
    "        # Replacing underscores with -'s to better fit real time drilling data\n",
    "        log_date = log_date.replace(\"_\", \"-\")\n",
    "        wellbore_name = wellbore_name.replace(\"_\", \"-\")\n",
    "\n",
    "        # Need to add columns to dataframes with the name and date\n",
    "        for i in range(table_num):\n",
    "            curr_df = df_list[i]\n",
    "            curr_cols = curr_df.columns\n",
    "            # Get flag for if it goes through to continue to the next iteration of the big path\n",
    "            log_date_list = [log_date] * curr_df.shape[0]\n",
    "            wellbore_name_list = [wellbore_name] * curr_df.shape[0]\n",
    "\n",
    "            if 'Start\\rtime' in curr_cols:\n",
    "                curr_df['log_date'] = log_date_list  # Fix repetative code\n",
    "                curr_df['wellbore_name'] = wellbore_name_list\n",
    "                operations_df_list.append(curr_df)  # We will want to continue\n",
    "                break  # The breaks are just to make the code more efficient (less iterations)\n",
    "            elif 'Start\\rtime' in curr_df.iloc[0].tolist():\n",
    "                new_df_cols = curr_df.iloc[0]\n",
    "                curr_df['log_date'] = log_date_list\n",
    "                curr_df['wellbore_name'] = wellbore_name_list\n",
    "                curr_df = curr_df.drop(curr_df.index[0]).rename(columns=new_df_cols)\n",
    "                operations_df_list.append(curr_df)\n",
    "                break\n",
    "\n",
    "    print(\"Number of operations dataframes sucessfully found: \" + str(len(operations_df_list)))\n",
    "    return operations_df_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3vioenmboGQh",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def combine_time_and_log_data(real_time_drilling_data, daily_log_data):\n",
    "    \"\"\"\n",
    "    Combines the real time drilling data with the operations table from\n",
    "    the daily log drilling data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    real_time_drilling_data : Pandas Dataframe\n",
    "        WITSML Realtime drilling time from the Volve dataset taken from Azure\n",
    "        Blob Storage converted into a Pandas Dataframe\n",
    "    daily_log_data : Pandas Dataframe\n",
    "        Operations data tables from the daily drilling reports in the Volve dataset\n",
    "        converted intoa Pandas Dataframe from Azure Blob Storage\n",
    "    \"\"\"\n",
    "\n",
    "    #Parse datetimes of TIME column into two seperate columns\n",
    "    time = \"TIME\"\n",
    "    # if time not in real_time_drilling_data:\n",
    "    #     time = \"Time\"\n",
    "    # daily_log_data.rename(columns={\"Start\\rtime\": \"Start_time\", \"End\\rtime\": \"End_time\"})\n",
    "\n",
    "    real_time_drilling_data['log_date'] = real_time_drilling_data[time].map(lambda x: x[:10])\n",
    "    real_time_drilling_data['current_time'] = real_time_drilling_data[time].map(lambda x: x[11:-1])\n",
    "    #Makes new wellbore_name column to be compatible with the daily_log_date\n",
    "    real_time_drilling_data['wellbore_name'] = real_time_drilling_data['nameWellbore'].map(lambda x: (x[:8]).replace(\"/\", \"_\"))\n",
    "\n",
    "    print(daily_log_data.head())\n",
    "    q = \"\"\"\n",
    "      SELECT *\n",
    "      FROM real_time_drilling_data rtdd\n",
    "      INNER JOIN daily_log_data dld \n",
    "        ON (rtdd.current_time BETWEEN\n",
    "          dld.Start_time AND dld.End_time)\n",
    "          AND (rtdd.log_date == dld.log_date)\n",
    "          AND (rtdd.wellbore_name == dld.wellbore_name)\n",
    "    \"\"\"\n",
    "\n",
    "    joined_df = sqldf(q)\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8Xol2kUSoIux",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def combine_drilling_and_formation_data(combined_data, formation_data):\n",
    "    \"\"\"\n",
    "    Combines the combined data with additional formation data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    combined_data : Pandas Dataframe\n",
    "        WITSML Realtime drilling time from the Volve dataset taken from Azure\n",
    "        Blob Storage converted into a Pandas Dataframe\n",
    "        + drilling report log data\n",
    "        \n",
    "    formation_data : Pandas Dataframe\n",
    "        formation dataframe with formation name and the top of formation depth\n",
    "    \"\"\"\n",
    "    formation_data[\"Well name\"] = formation_data[\"Well name\"].str.lstrip('NO ')\n",
    "    formation_data[\"Well name\"] = formation_data[\"Well name\"].str.replace(\"/\",\"_\")\n",
    "    formation_data = formation_data[[\"Well name\",\"Surface name\",\"MD\",\"TVD\"]]\n",
    "    formation_data = formation_data.rename(columns={\"Well name\":\"wellbore_name\",\"Surface name\":\"Formation\",\"MD\": \"MD_Top\", \"TVD\": \"TVD_Top\"})\n",
    "    formation_data[\"MD_Bottom\"] = np.nan\n",
    "    formation_data[\"TVD_Bottom\"] = np.nan\n",
    "    \n",
    "    fd_unique_md = formation_data[\"MD_Top\"].unique()\n",
    "    fd_unique_tvd = formation_data[\"TVD_Top\"].unique()\n",
    "\n",
    "    for i in formation_data.index:\n",
    "        for j in range (len(fd_unique_md)-1):\n",
    "            if formation_data[\"MD_Top\"][i] == fd_unique_md[j]:\n",
    "                formation_data.loc[i,\"MD_Bottom\"] = fd_unique_md[j+1]\n",
    "\n",
    "    for i in formation_data.index:\n",
    "        for j in range (len(fd_unique_tvd)-1):    \n",
    "            if formation_data[\"TVD_Top\"][i] == fd_unique_tvd[j]:\n",
    "                formation_data.loc[i,\"TVD_Bottom\"] = fd_unique_tvd[j+1]        \n",
    "    formation_data[[\"MD_Bottom\",\"TVD_Bottom\"]] = formation_data[[\"MD_Bottom\",\"TVD_Bottom\"]].fillna(10000)\n",
    "    \n",
    "    q = \"\"\"\n",
    "      SELECT *\n",
    "      FROM combined_data cd\n",
    "        INNER JOIN formation_data fd\n",
    "          ON cd.DMEA >= fd.MD_Top\n",
    "          AND cd.DMEA < fd.MD_Bottom\n",
    "          AND cd.wellbore_name == fd.wellbore_name\n",
    "    \"\"\"\n",
    "\n",
    "    joined_df = sqldf(q)\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XfyVwUL5om2c",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def fetch_well_data(well_names, out_dir, path_checker_csv=None, override=False):\n",
    "    \"\"\"\n",
    "    Fetches drilling data and daily log data and merges them into a singular \"..._combined.csv\" file\n",
    "    :param well_names: list of well names to fetch data for\n",
    "    :type well_names: list[str]\n",
    "    :param out_dir: path to an existing directory to output files\n",
    "    :type out_dir: str\n",
    "    :param path_checker_csv: path to csv file containing valid drilling file paths\n",
    "    :type path_checker_csv: str\n",
    "    \"\"\"\n",
    "\n",
    "    sas_dict = fetch_sas_paths(path_checker_csv)\n",
    "    path_tuple = [(w, w_paths) for w, w_paths in sas_dict.items() if w in well_names]\n",
    "\n",
    "    for w, w_paths in path_tuple:\n",
    "\n",
    "        print(\"Fetching files for\", w)\n",
    "        out_paths = fetch_all_drilling_data(out_dir, sas_dict,\n",
    "                                            w, override)  # Gets output paths for the csv's for real time drilling data\n",
    "        html_path_list = fetch_html_daily_drilling_logs(out_dir, sas_dict, wellbore_name=w, log_date=None)\n",
    "        # path_list = fetch_daily_drilling_logs(out_dir, sas_dict, w) #Gets path_list for the pdfs\n",
    "        print(\"Finished fetching drilling and HTML files\")\n",
    "        log_df_concatenated = convert_html_to_dataframe(html_path_list, delete=True)\n",
    "        # log_df_list = get_drill_log_tables(path_list, delete=True)\n",
    "        # log_df_concatenated = pd.concat(log_df_list, ignore_index=True)\n",
    "        # run_dir = os.path.join(out_dir, 'VOLVE_INVENTORY.xlsx')\n",
    "        # run_path = fetch_wellbore_run_data(run_dir)  # This will be some path\n",
    "        run_path = '/content/drive/My Drive/Volve Project Shared Folder/VOLVE_INVENTORY.xlsx'\n",
    "        run_df_list = group_wellbore_run_data(run_path, [w])  # This function takes in a list\n",
    "        run_df_combined = pd.concat(run_df_list, ignore_index=True)\n",
    "\n",
    "        print(\"log_df_concatenated cols: \" + str(log_df_concatenated))\n",
    "\n",
    "        #get formation dataframe\n",
    "        formation_df_path = '/content/drive/My Drive/Volve Project Shared Folder/well_picks/Well_picks_' + w + '.csv'\n",
    "        formation_df = pd.read_csv(formation_df_path)\n",
    "\n",
    "        # out_path_list = []\n",
    "        for out_path in out_paths:\n",
    "            drill_df = pd.read_csv(out_path)\n",
    "\n",
    "            com_df = combine_time_and_html_log_data(drill_df, log_df_concatenated)\n",
    "            com_df = combine_drilling_and_formation_data(com_df, formation_df)\n",
    "            # out_path_list.append(com_df)\n",
    "            os.remove(out_path)\n",
    "\n",
    "            filename, file_extension = os.path.splitext(out_path)\n",
    "            csv_out = os.path.join(out_dir, filename + '_combined.csv')\n",
    "            com_df.to_csv(csv_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_ryIcuGmuM2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Replace first input parameter with well names or list of well names that you want to fetch the combined dataset\n",
    "#Replace second input parameter of fetch_well_data with path of where you want to write the combined dataset\n",
    "#Replace third input parameter of fetch_well_data with path to Drilling_Data_Path.csv\n",
    "\n",
    "fetch_well_data(['15_9-F-15'], '/content/drive/My Drive/Volve Project Shared Folder/15_9-F-15 Dataset',path_checker_csv=\"/content/drive/My Drive/Volve Project Shared Folder/Drilling_Data_Path.csv\",\n",
    "                override=False)\n",
    "\n",
    "# fetch_well_data(['15_9-F-15','15_9-F-15 A'], '/content/drive/My Drive/Volve Project Shared Folder/',path_checker_csv=\"/content/drive/My Drive/Volve Project Shared Folder/Drilling_Data_Path.csv\",\n",
    "#                 override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "znbx-xGeV_D2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Get Combined Data Set.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
